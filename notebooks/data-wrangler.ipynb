{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "## Import and data sources\n",
    "import numpy as np                              \n",
    "import pandas as pd                             \n",
    "import urllib.request as req                    \n",
    "import ast\n",
    "import matplotlib.pyplot as plt            \n",
    "import seaborn as sns                           \n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# To supress warnings from shap\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "N_THREADS = 20  ## NOTE: The number of threads for parallel runs. Modify as needed.\n",
    "os.environ['OMP_NUM_THREADS'] = str(N_THREADS)\n",
    "    \n",
    "# sns.set(font_scale=0.9, style='white')\n",
    "\n",
    "census_key =  ## The key from https://www.census.gov/data/developers.html \"Request a key\".\n",
    "\n",
    "# APIs from the US Census Bureau\n",
    "population_density_url = 'https://api.census.gov/data/2019/pep/population?get=DENSITY&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "above_65_url = 'https://api.census.gov/data/2019/acs/acs5/subject?get=GEO_ID,NAME,S0101_C02_030E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "transport_url = 'https://api.census.gov/data/2019/acs/acs5?get=GEO_ID,NAME,B08006_001E,B08006_003E,B08006_017E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "race_url = 'https://api.census.gov/data/2019/acs/acs5/profile?get=GEO_ID,NAME,DP05_0070E,DP05_0077E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "poverty_url = 'https://api.census.gov/data/2019/acs/acs5/subject?get=GEO_ID,NAME,S1701_C03_001E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "income_url = 'https://api.census.gov/data/2019/acs/acs5/subject?get=GEO_ID,NAME,S1902_C03_019E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "employed_url = 'https://api.census.gov/data/2019/acs/acs5/subject?get=GEO_ID,NAME,S2301_C03_001E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "unemployment_url = 'https://api.census.gov/data/2019/acs/acs5/subject?get=GEO_ID,NAME,S2301_C04_001E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "mean_commute_url = 'https://api.census.gov/data/2019/acs/acs5/subject?get=GEO_ID,NAME,S0801_C01_046E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "labour_url = 'https://api.census.gov/data/2019/acs/acs5/profile?get=GEO_ID,NAME,DP03_0028PE,DP03_0030PE,DP03_0031PE&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "insurance_url = 'https://api.census.gov/data/2019/acs/acs5/profile?get=GEO_ID,NAME,DP03_0099PE&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "gini_url = 'https://api.census.gov/data/2019/acs/acs5?get=GEO_ID,NAME,B19083_001E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "education_url = 'https://api.census.gov/data/2019/acs/acs5/subject?get=GEO_ID,NAME,S1501_C02_012E&in=state:*&for=county:*&key={}'.format(census_key)\n",
    "\n",
    "# links to the JHU GitHub\n",
    "jhu_deaths = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "jhu_confirmed = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "\n",
    "# comorbidities data stored locally\n",
    "ccc_data = '../data/CDC/CDC_PMID_32233970.xlsx'\n",
    "any_data = '../data/CDC/CDC_MMWR_6929.xlsx'\n",
    "\n",
    "nensemble = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sectorAnalyze(df, df_c, useDensity=False, type=0, scale=False, N=100, run_BDT=False):\n",
    "    ''' A function to prepare the data for the BDT analysis\n",
    "        arguments:\n",
    "            df: the dataframe for the socio-economic conditions\n",
    "            df_c: the dataframe for covid-19 prevalence\n",
    "            useDensity: boolean to determine whether density is used in the analysis\n",
    "            type:\n",
    "                0: death rate\n",
    "                1: confirmed case rate\n",
    "                2: fatality rate\n",
    "            scale: boolean to toggle scaling of the features\n",
    "            N: The number of BDTs in an ensemble\n",
    "            run_BDT: boolean to determin whether to call run_BDT() after cleaning the data\n",
    "        return:\n",
    "            df: the combined dataframe\n",
    "    '''\n",
    "    df = pd.merge(df, df_c, on='FIPS')\n",
    "    if type == 0: df['death rate'] = np.log10(df['deaths'].clip(lower=0) / df['Population'] * 100000. + 1.)\n",
    "    if type == 1: df['confirmed rate'] = np.log10(df['confirmed'].clip(lower=0) / df['Population'] * 100000. + 1.)\n",
    "    if type == 2: df['fatality rate'] = df['deaths'].clip(lower=0) / df['confirmed_base'] * 100\n",
    "    if useDensity: df.drop(columns=['Population', 'deaths', 'confirmed', 'confirmed_base'], inplace=True)\n",
    "    else: df.drop(columns=['Population', 'deaths', 'confirmed', 'confirmed_base', 'Density'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def makeCorrelationPlot(df, df_c, filename='', cbar=False, only_corr=True):\n",
    "    ''' A function to make the correlation plot using sns\n",
    "        arguments:\n",
    "            df: the dataframe to make the plot from\n",
    "            cbar: boolean to decide whether to plot the colourbar\n",
    "        return:\n",
    "            fig: the figure\n",
    "            ax: the axes\n",
    "    '''\n",
    "    df_plot = df.copy(deep=True)\n",
    "    df_plot_0 = sectorAnalyze(df_plot, df_c, useDensity=True, type=0, run_BDT=False)\n",
    "    df_plot_1 = sectorAnalyze(df_plot, df_c, useDensity=True, type=1, run_BDT=False)\n",
    "    df_plot_2 = sectorAnalyze(df_plot, df_c, useDensity=True, type=2, run_BDT=False)\n",
    "    df_plot_2.drop(df_plot_2[df_plot_2['fatality rate']==np.inf].index, inplace=True)\n",
    "    df_plot = df_plot_1.copy(deep=True)\n",
    "    df_plot['death rate'] = df_plot_0['death rate']\n",
    "    df_plot['fatality rate'] = df_plot_2['fatality rate']\n",
    "    df_plot.Density = 10**df_plot.Density\n",
    "    df_plot['confirmed rate'] = 10**df_plot['confirmed rate'] - 1.\n",
    "    df_plot['death rate'] = 10**df_plot['death rate'] - 1.\n",
    "    df_plot['fatality rate'] = df_plot['fatality rate']\n",
    "    \n",
    "    if only_corr:\n",
    "        return df_plot.corr(method='pearson')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(15.5, 13))\n",
    "        mask = np.zeros((df_plot.shape[1],df_plot.shape[1]))\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        np.fill_diagonal(mask,0)\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "        cbar_kws = {\"aspect\": 50, \"ticks\": [-1.0,-0.8,-0.6,-0.4,-0.2,0.0,0.2,0.4,0.6,0.8,1.0]}\n",
    "        corr = df_plot.corr(method='pearson')\n",
    "        sns.heatmap(round(corr, 2)+0., mask=mask, annot=True, cmap=cmap, vmin=-1.0, vmax=1.0, cbar_kws=cbar_kws, cbar=cbar, annot_kws={\"fontsize\":10})\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(labelsize=12)\n",
    "        fig.set_size_inches(9.,9.)\n",
    "        plt.tight_layout()\n",
    "        if filename != '': plt.savefig(filename)\n",
    "        return fig, ax, corr\n",
    "\n",
    "def abs_shap(df_shap, df, lim=0.5):\n",
    "    ''' A function to plot the bar plot for the mean abs SHAP values\n",
    "        arguments:\n",
    "            df_shap: the dataframe of the SHAP values\n",
    "            df: the dataframe for the feature values for which the SHAP values have been determined\n",
    "    '''\n",
    "    # Make a copy of the input data\n",
    "    shap_v = pd.DataFrame(df_shap)\n",
    "    feature_list = df.columns\n",
    "    shap_v.columns = feature_list\n",
    "    df_v = df.copy().reset_index().drop('index',axis=1)\n",
    "\n",
    "    # Determine the correlation in order to plot with different colors\n",
    "    corr_list = list()\n",
    "    for i in feature_list:\n",
    "        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n",
    "        corr_list.append(b)\n",
    "    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)\n",
    "    # Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient\n",
    "    corr_df.columns  = ['Variable','Corr']\n",
    "    corr_df['Sign'] = np.where(corr_df['Corr']>0,'#da3b46','#3f7f93')\n",
    "\n",
    "    # Plot it\n",
    "    shap_abs = np.abs(shap_v)\n",
    "    k=pd.DataFrame(shap_abs.mean()).reset_index()\n",
    "    k.columns = ['Variable','SHAP_abs']\n",
    "    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
    "    k2 = k2.sort_values(by='SHAP_abs',ascending = True)\n",
    "    colorlist = k2['Sign']\n",
    "    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(9,6),legend=False, zorder=2)\n",
    "    ax.set_xlabel(\"Mean(|SHAP Value|) (Red = Positive Impact, Blue = Negative Impact)\")\n",
    "    ax.set_ylabel('')\n",
    "    # Switch off ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "    plt.xlim(0, lim)\n",
    "    # Draw vertical axis lines\n",
    "    vals = ax.get_xticks()\n",
    "    for tick in vals:\n",
    "        ax.axvline(x=tick, linestyle='dashed', alpha=0.4, color='#808080', zorder=1)\n",
    "    \n",
    "def get_census_data(url):\n",
    "    ''' A function to get the data from USA Census Bureau\n",
    "        arguments:\n",
    "            url: the url for the API call\n",
    "        return:\n",
    "            df: the dataframe with the pulled data\n",
    "    '''\n",
    "    df = pd.DataFrame(ast.literal_eval(req.urlopen(req.Request(url)).read().decode('utf8').replace('null', '\"1\"')))\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df['FIPS'] = df.pop('GEO_ID').apply(lambda x: int(x[-5:]))\n",
    "    return df\n",
    "\n",
    "sns.set(font_scale=1.25, style='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "## The density data from census.gov. We are removing the counties with density = 0 (Puerto Rico)\n",
    "density = pd.DataFrame(ast.literal_eval(req.urlopen(req.Request(population_density_url)).read().decode('utf8').replace('null', '\"0\"')))\n",
    "density.columns = density.iloc[0]\n",
    "density = density[1:]\n",
    "density.DENSITY = density.DENSITY.astype('float32')\n",
    "density.state = density.state.astype('string')\n",
    "density.county = density.county.astype('string')\n",
    "density['FIPS'] = density['state'] + density['county']\n",
    "density.FIPS = density.FIPS.astype('int32')\n",
    "density = density.drop(labels=['state', 'county'], axis=1)\n",
    "columns = density.columns.tolist()\n",
    "columns = columns[::-1]\n",
    "density = density[columns]\n",
    "density.drop(density[density['DENSITY'] == 0].index , inplace=True)\n",
    "density.DENSITY = np.log10(density.DENSITY)\n",
    "density.rename(columns={'DENSITY': 'Density'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## The JHU data\n",
    "def getJHUdata(date, basedate, cut=True):\n",
    "    deaths = pd.read_csv(jhu_deaths)\n",
    "    deaths.drop(deaths[(deaths['FIPS'] < 1000) | (deaths['FIPS'] > 57000)].index , inplace=True)\n",
    "    deaths.dropna(inplace=True)\n",
    "\n",
    "    confirmed = pd.read_csv(jhu_confirmed)\n",
    "    confirmed.drop(confirmed[(confirmed['FIPS'] < 1000) | (confirmed['FIPS'] > 57000)].index , inplace=True)\n",
    "    confirmed.dropna(inplace=True)\n",
    "\n",
    "    deaths_df = deaths[['FIPS', 'Population', date[0], date[1]]]\n",
    "    deaths_df[date[1]] = deaths_df[date[1]] - deaths_df[date[0]]\n",
    "    deaths_df.drop(columns=[date[0]], inplace=True)\n",
    "    \n",
    "    confirmed_df = confirmed[['FIPS', basedate[0], basedate[1], date[0], date[1]]]\n",
    "    confirmed_df[date[1]] = confirmed_df[date[1]] - confirmed_df[date[0]]\n",
    "    confirmed_df[basedate[1]] = confirmed_df[basedate[1]] - confirmed_df[basedate[0]]\n",
    "    confirmed_df.drop(columns=[date[0], basedate[0]], inplace=True)\n",
    "\n",
    "    ## Death rate computation with the JHU data\n",
    "    df_covid19 = deaths_df.copy()\n",
    "    df_covid19.rename(columns={df_covid19.columns[-1]: 'deaths'}, inplace=True)\n",
    "    df_covid19['confirmed'] = confirmed_df[date[1]]\n",
    "    df_covid19['confirmed_base'] = confirmed_df[basedate[1]]\n",
    "    df_covid19.dropna(inplace=True)\n",
    "    df_covid19.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ## Cleaning out low statistics data and merging JHU with density data\n",
    "    if cut: \n",
    "        df_cut = df_covid19.drop(df_covid19[(df_covid19['confirmed'] < 1) & (df_covid19['deaths'] < 1)].index)\n",
    "        df_cut.reset_index(drop=True, inplace=True)\n",
    "        return df_cut\n",
    "    else:\n",
    "        return df_covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FIPS and state names\n",
    "df_census = get_census_data(above_65_url)\n",
    "df_census = df_census[['FIPS', 'NAME']]\n",
    "df_census['State'] = df_census['NAME'].str.split(', ').str[1]\n",
    "df_census['County'] = df_census.pop('NAME').str.split(', ').str[0]\n",
    "df_census = df_census.sort_values(df_census.columns[0], ignore_index=True)\n",
    "df_census = pd.merge(df_census, density, on='FIPS')\n",
    "\n",
    "# 2019 5 year ACS data on Age\n",
    "dft = get_census_data(above_65_url)\n",
    "dft = dft[['FIPS', 'S0101_C02_030E']]\n",
    "dft.rename(columns={'S0101_C02_030E': 'Senior Citizen'}, inplace=True)\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Senior Citizen'] = df_census['Senior Citizen'].apply(lambda x: float(x))/100.\n",
    "\n",
    "# 2019 5 year ACS data on transport used\n",
    "dft = get_census_data(transport_url)\n",
    "dft['Transit-1'] = 1. - (dft['B08006_003E'].astype('float')+dft['B08006_017E'].astype('float'))/dft['B08006_001E'].astype('float')\n",
    "dft = dft[['FIPS', 'Transit-1']]\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Transit'] = df_census.pop('Transit-1')\n",
    "df_census.at[df_census[df_census['FIPS']==35039].index, 'Transit'] = 0.159 ## Imputed value from 2011 since values are null in 2019\n",
    "\n",
    "# 2019 5 year ACS data on non-white\n",
    "dft = get_census_data(race_url)\n",
    "# dft['Non-White'] = 1. - dft['B02001_002E'].astype('float')/dft['B02001_001E'].astype('float')\n",
    "dft['Non-White'] = 1. - dft['DP05_0077E'].astype('float')/dft['DP05_0070E'].astype('float')\n",
    "dft = dft[['FIPS', 'Non-White']]\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "\n",
    "# 2019 5 year ACS data on Poverty\n",
    "dft = get_census_data(poverty_url)\n",
    "dft = dft[['FIPS', 'S1701_C03_001E']]\n",
    "dft.rename(columns={'S1701_C03_001E': 'Poverty-1'}, inplace=True)\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Poverty'] = df_census.pop('Poverty-1').apply(lambda x: float(x))/100.\n",
    "df_census.at[df_census[df_census['FIPS']==35039].index, 'Poverty'] = 0.237 ## Imputed value from 2011 since values are null in 2019\n",
    "\n",
    "# 2019 5 year ACS data on Income per capita\n",
    "dft = get_census_data(income_url)\n",
    "dft = dft[['FIPS', 'S1902_C03_019E']]\n",
    "dft.rename(columns={'S1902_C03_019E': 'Income/Capita'}, inplace=True)\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Income/Capita'] = df_census['Income/Capita'].apply(lambda x: float(x))\n",
    "df_census.at[df_census[df_census['FIPS']==35039].index, 'Income/Capita'] = 19678 ## Imputed value from 2011 since values are null in 2019\n",
    "\n",
    "# 2019 5 year ACS data on Employment\n",
    "dft = get_census_data(employed_url)\n",
    "dft = dft[['FIPS', 'S2301_C03_001E']]\n",
    "dft.rename(columns={'S2301_C03_001E': 'Employed-1'}, inplace=True)\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Employed'] = df_census.pop('Employed-1').apply(lambda x: float(x))/100.\n",
    "df_census.at[df_census[df_census['FIPS']==35039].index, 'Employed'] = 0.382287 ## Imputed value from 2011 since values are null in 2019\n",
    "\n",
    "# 2019 5 year ACS data on Unemployment\n",
    "dft = get_census_data(unemployment_url)\n",
    "dft = dft[['FIPS', 'S2301_C04_001E']]\n",
    "dft.rename(columns={'S2301_C04_001E': 'Unemployment-1'}, inplace=True)\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Unemployment'] = df_census.pop('Unemployment-1').apply(lambda x: float(x))/100.\n",
    "df_census.at[df_census[df_census['FIPS']==35039].index, 'Unemployment'] = 0.114 ## Imputed value from 2011 since values are null in 2019\n",
    "\n",
    "# 2019 5 year ACS data on Mean Commute in minutes\n",
    "dft = get_census_data(mean_commute_url)\n",
    "dft = dft[['FIPS', 'S0801_C01_046E']]\n",
    "dft.rename(columns={'S0801_C01_046E': 'MeanCommute'}, inplace=True)\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['MeanCommute'] = df_census['MeanCommute'].apply(lambda x: float(x))\n",
    "df_census.at[df_census[df_census['FIPS']==35039].index, 'MeanCommute'] = 26.2 ## Imputed value from 2011 since values are null in 2019\n",
    "df_census.at[df_census[df_census['FIPS']==48301].index, 'MeanCommute'] = 25.4 ## Imputed value from 2011 since values are null in 2019\n",
    "\n",
    "# 2019 5 year ACS data on Labour\n",
    "dft = get_census_data(labour_url)\n",
    "dft['Labour'] = (dft['DP03_0028PE'].astype('float') + dft['DP03_0030PE'].astype('float') + dft['DP03_0031PE'].astype('float'))/100.\n",
    "dft = dft[['FIPS', 'Labour']]\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census.at[df_census[df_census['FIPS']==35039].index, 'Labour'] = 0.271 + 0.108 + 0.07 ## Imputed value from 2011 since values are null in 2019\n",
    "\n",
    "# 2018 5 year ACS data on Health Insurance\n",
    "dft = get_census_data(insurance_url)\n",
    "dft = dft[['FIPS', 'DP03_0099PE']]\n",
    "dft.rename(columns={'DP03_0099PE': 'Uninsured'}, inplace=True)\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Uninsured'] = df_census.pop('Uninsured').apply(lambda x: float(x))/100.\n",
    "\n",
    "# 2019 5 year ACS data on Gini Index\n",
    "dft = get_census_data(gini_url)\n",
    "dft = dft[['FIPS', 'B19083_001E']]\n",
    "dft.rename(columns={'B19083_001E': 'Gini'}, inplace=True)\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Gini'] = df_census['Gini'].astype('float')\n",
    "df_census.at[df_census[df_census['FIPS']==35039].index, 'Gini'] = 0.4290 ## Imputed value from 2011 since values are null in 2019\n",
    "\n",
    "# 2018 5 year ACS data on Health Insurance\n",
    "dft = get_census_data(education_url)\n",
    "dft = dft[['FIPS', 'S1501_C02_012E']] ##\n",
    "dft.rename(columns={'S1501_C02_012E': 'Education'}, inplace=True) ##\n",
    "df_census = pd.merge(df_census, dft, on='FIPS')\n",
    "df_census['Education'] = df_census.pop('Education').apply(lambda x: float(x))/100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comorbidities data CCC\n",
    "df_ccc_data = pd.read_excel(ccc_data, engine='openpyxl')\n",
    "df_ccc = df_ccc_data[['FIPS', 'ccc_measure', 'Beneficiaries', 'FFS_bene', 'cntypop', 'urban_code']]\n",
    "df_ccc.drop(df_ccc_data[df_ccc_data['FIPS'] == 51515].index, inplace=True) # Bedford City has been merged with Bedford County\n",
    "df_ccc.at[df_ccc[df_ccc['FIPS']==2270].index, 'FIPS'] = 2158 # Wade Hampton Census Area is now Kusilvak Census Area\n",
    "df_ccc.at[df_ccc[df_ccc['FIPS']==46113].index, 'FIPS'] = 46102 # Shannon County is now Oglala Lakota County\n",
    "\n",
    "# Comorbidities data any-condition\n",
    "df_any_data = pd.read_excel(any_data, engine='openpyxl')\n",
    "df_any = df_any_data[['FIPS', 'anycondition_prevalence']]\n",
    "\n",
    "df_census = pd.merge(df_census, df_ccc, on='FIPS')\n",
    "df_census = pd.merge(df_census, df_any, on='FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Collecting all the necessary features into one dataframe\n",
    "df_census_curated = pd.DataFrame()\n",
    "df_census_curated['FIPS'] = df_census['FIPS']\n",
    "df_census_curated['State'] = df_census['State']\n",
    "df_census_curated['Density'] = df_census['Density']\n",
    "df_census_curated['Non-White'] = df_census['Non-White']\n",
    "df_census_curated['Poverty'] = df_census['Poverty']\n",
    "df_census_curated['Income'] = df_census['Income/Capita']\n",
    "df_census_curated['Unemployment'] = df_census['Unemployment']\n",
    "df_census_curated['Uninsured'] = df_census['Uninsured']\n",
    "df_census_curated['Employed'] = df_census['Employed']\n",
    "df_census_curated['Labor'] = df_census['Labour']\n",
    "df_census_curated['Transit'] = df_census['Transit']\n",
    "df_census_curated['Mean Commute'] = df_census['MeanCommute']\n",
    "df_census_curated['Senior Citizen'] = df_census['Senior Citizen']\n",
    "df_census_curated['Gini'] = df_census['Gini']\n",
    "# df_census_curated['Education'] = df_census['Education']\n",
    "# df_census_curated['CCC'] = df_census['ccc_measure']\n",
    "df_census_curated['Comorbidities'] = df_census['anycondition_prevalence'].apply(lambda x: float(x))/100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "## The East Coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%% Density Analysis\n"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering out the states to be analysed.\n",
    "states = ['District of Columbia', 'New Jersey', 'Rhode Island', 'Massachusetts', 'Connecticut', 'Maryland', 'Delaware', 'New York']\n",
    "\n",
    "df_density_curated = pd.DataFrame()\n",
    "for state in states:\n",
    "    df_density_curated = pd.concat([df_density_curated, df_census_curated[df_census_curated['State'] == state]])\n",
    "df_density_curated.drop(columns='State', inplace=True)\n",
    "\n",
    "\n",
    "# Generating the plot for the correlation matrix for February to July\n",
    "df_cut = getJHUdata(['2/15/20', '7/15/20'], ['2/5/20', '7/5/20'])\n",
    "corr = makeCorrelationPlot(df_density_curated, df_cut)\n",
    "corr.to_csv('../data/Correlations/East_Coast_February_July.csv')\n",
    "\n",
    "df_10 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=0)\n",
    "df_11 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=1)\n",
    "df_11['death rate'] = df_10['death rate']\n",
    "df_11.to_csv('../data/East_Coast_February_July.csv')\n",
    "\n",
    "# Generating the plot for the correlation matrix for July to January\n",
    "df_cut = getJHUdata(['7/15/20', '1/15/21'], ['7/5/20', '1/5/21'])\n",
    "corr = makeCorrelationPlot(df_density_curated, df_cut)\n",
    "corr.to_csv('../data/Correlations/East_Coast_July_January.csv')\n",
    "\n",
    "df_20 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=0)\n",
    "df_21 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=1)\n",
    "df_21['death rate'] = df_20['death rate']\n",
    "df_21.to_csv('../data/East_Coast_July_January.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## The Southern States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%% Density Analysis\n"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering out the states to be analysed.\n",
    "states = ['Alabama', 'Arkansas', 'Florida', 'Georgia', 'Kentucky', 'Louisiana',\n",
    "          'Mississippi', 'North Carolina', 'Oklahoma', 'South Carolina', 'Tennessee', 'Texas', 'Virginia',\n",
    "          'West Virginia']\n",
    "\n",
    "df_density_curated = pd.DataFrame()\n",
    "for state in states:\n",
    "    df_density_curated = pd.concat([df_density_curated, df_census_curated[df_census_curated['State'] == state]])\n",
    "df_density_curated.drop(columns='State', inplace=True)\n",
    "\n",
    "\n",
    "# Generating the plot for the correlation matrix from February to July\n",
    "df_cut = getJHUdata(['2/15/20', '7/15/20'], ['2/5/20', '7/5/20'])\n",
    "corr = makeCorrelationPlot(df_density_curated, df_cut)\n",
    "corr.to_csv('../data/Correlations/Southern_States_February_July.csv')\n",
    "\n",
    "df_10 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=0)\n",
    "df_11 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=1)\n",
    "df_11['death rate'] = df_10['death rate']\n",
    "df_11.to_csv('../data/Southern_States_February_July.csv')\n",
    "\n",
    "# Generating the plot for the correlation matrix for July to January\n",
    "df_cut = getJHUdata(['7/15/20', '1/15/21'], ['7/5/20', '1/5/21'])\n",
    "corr = makeCorrelationPlot(df_density_curated, df_cut)\n",
    "corr.to_csv('../data/Correlations/Southern_States_July_January.csv')\n",
    "\n",
    "df_20 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=0)\n",
    "df_21 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=1)\n",
    "df_21['death rate'] = df_20['death rate']\n",
    "df_21.to_csv('../data/Southern_States_July_January.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## The West Coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%% Density Analysis\n"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering out the states to be analysed.\n",
    "states = ['California', 'Oregon', 'Washington']\n",
    "\n",
    "df_density_curated = pd.DataFrame()\n",
    "for state in states:\n",
    "    df_density_curated = pd.concat([df_density_curated, df_census_curated[df_census_curated['State'] == state]])\n",
    "df_density_curated.drop(columns='State', inplace=True)\n",
    "\n",
    "\n",
    "# Generating the plot for the correlation matrix from February to July\n",
    "df_cut = getJHUdata(['2/15/20', '7/15/20'], ['2/5/20', '7/5/20'])\n",
    "corr = makeCorrelationPlot(df_density_curated, df_cut)\n",
    "corr.to_csv('../data/Correlations/West_Coast_February_July.csv')\n",
    "\n",
    "df_10 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=0)\n",
    "df_11 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=1)\n",
    "df_11['death rate'] = df_10['death rate']\n",
    "df_11.to_csv('../data/West_Coast_February_July.csv')\n",
    "\n",
    "# Generating the plot for the correlation matrix for July to January\n",
    "df_cut = getJHUdata(['7/15/20', '1/15/21'], ['7/5/20', '1/5/21'])\n",
    "corr = makeCorrelationPlot(df_density_curated, df_cut)\n",
    "corr.to_csv('../data/Correlations/West_Coast_July_January.csv')\n",
    "\n",
    "df_20 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=0)\n",
    "df_21 = sectorAnalyze(df_density_curated, df_cut, useDensity=True, type=1)\n",
    "df_21['death rate'] = df_20['death rate']\n",
    "df_21.to_csv('../data/West_Coast_July_January.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "__________________\n",
    "## Panel Data (Weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPanel(df_c, period, region, norm=True, weekly=False, datatype='confirmed_cases'):\n",
    "    confirmed = pd.read_csv(jhu_confirmed).drop(columns=['UID', 'iso2', 'iso3', 'code3', 'Admin2', 'Province_State', 'Country_Region', 'Lat', 'Long_', 'Combined_Key'])\n",
    "    deaths = pd.read_csv(jhu_deaths).drop(columns=['UID', 'iso2', 'iso3', 'code3', 'Admin2', 'Province_State', 'Country_Region', 'Lat', 'Long_', 'Combined_Key'])\n",
    "    if weekly: \n",
    "        confirmed = pd.concat([confirmed['FIPS'], confirmed.drop(columns=['FIPS']).diff(axis=1).rolling(window=7, axis=1).sum().dropna(axis=1)], axis=1)\n",
    "        deaths = pd.concat([deaths[['FIPS', 'Population']], deaths.drop(columns=['FIPS', 'Population']).diff(axis=1).rolling(window=7, axis=1).sum().dropna(axis=1)], axis=1)\n",
    "\n",
    "    if datatype == 'confirmed_cases':\n",
    "        df = pd.merge(confirmed, deaths[['FIPS', 'Population']], on='FIPS')\n",
    "    elif datatype == 'death_rate':\n",
    "        df = deaths\n",
    "        \n",
    "    df_1 = df.pop('FIPS')\n",
    "    if norm: df = df.div(df.Population/100000, axis=0)\n",
    "    df = pd.concat([df_1, df], axis=1).drop(columns=['Population'])\n",
    "    df = df.dropna()[['FIPS']+period].set_index('FIPS').stack()\n",
    "    \n",
    "    \n",
    "    panel = df_c.set_index('FIPS').join(pd.DataFrame(df, columns=[datatype]), how='inner').reset_index(level=1)\n",
    "    panel['week'] = (pd.to_datetime(panel.level_1).dt.week - 7) % 53 + 7\n",
    "    panel = panel.drop(columns='level_1')\n",
    "    panel['region'] = region\n",
    "    return panel\n",
    "\n",
    "def mergePanels(df_c, period, norm=True, weekly=False, datatype='confirmed_cases'):\n",
    "    # Filtering out the East Coast States.\n",
    "    states_list = [['District of Columbia', 'New Jersey', 'Rhode Island', 'Massachusetts', 'Connecticut', 'Maryland', 'Delaware', 'New York'],\n",
    "                   ['Alabama', 'Arkansas', 'Florida', 'Georgia', 'Kentucky', 'Louisiana',\n",
    "                    'Mississippi', 'North Carolina', 'Oklahoma', 'South Carolina', 'Tennessee', 'Texas', 'Virginia',\n",
    "                    'West Virginia'],\n",
    "                   ['California', 'Oregon', 'Washington']]\n",
    "    regions = ['east', 'south', 'west']\n",
    "    \n",
    "    panel_list = []\n",
    "    for i in range(len(regions)):\n",
    "        df_density_curated = pd.DataFrame()\n",
    "        for state in states_list[i]:\n",
    "            df_density_curated = pd.concat([df_density_curated, df_c[df_c['State'] == state]])\n",
    "        df_density_curated.drop(columns='State', inplace=True)\n",
    "\n",
    "        panel_list.append(getPanel(df_density_curated, period=period, region=regions[i], norm=norm, weekly=weekly, datatype=datatype))\n",
    "\n",
    "    return pd.concat(panel_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Normalized Weekly Cases (confirmed cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the weeks\n",
    "per_1 = []\n",
    "for w in range(22):\n",
    "    week = 6 + w\n",
    "    year = 2020\n",
    "    date = datetime.date(year, 1, 1) + relativedelta(weeks=+week) + relativedelta(days=+5)\n",
    "    per_1.append(date.strftime(\"%-m/%-d/%y\"))\n",
    "    \n",
    "panel_1 = mergePanels(df_census_curated, per_1, weekly=True)\n",
    "panel_1.to_csv('../data/panel_merged_period_1_weekly_norm_diff_confirmed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the weeks\n",
    "per_2 = []\n",
    "for w in range(22, 48):\n",
    "    week = 6 + w\n",
    "    year = 2020\n",
    "    date = datetime.date(year, 1, 1) + relativedelta(weeks=+week) + relativedelta(days=+5)\n",
    "    per_2.append(date.strftime(\"%-m/%-d/%y\"))\n",
    "    \n",
    "panel_2 = mergePanels(df_census_curated, per_2, weekly=True)\n",
    "panel_2.to_csv('../data/panel_merged_period_2_weekly_norm_diff_confirmed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalized Weekly Cases (death rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the weeks\n",
    "per_1 = []\n",
    "for w in range(22):\n",
    "    week = 6 + w\n",
    "    year = 2020\n",
    "    date = datetime.date(year, 1, 1) + relativedelta(weeks=+week) + relativedelta(days=+5)\n",
    "    per_1.append(date.strftime(\"%-m/%-d/%y\"))\n",
    "    \n",
    "panel_1 = mergePanels(df_census_curated, per_1, weekly=True, datatype='death_rate')\n",
    "panel_1.to_csv('../data/panel_merged_period_1_weekly_norm_diff_deaths.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the weeks\n",
    "per_2 = []\n",
    "for w in range(22, 48):\n",
    "    week = 6 + w\n",
    "    year = 2020\n",
    "    date = datetime.date(year, 1, 1) + relativedelta(weeks=+week) + relativedelta(days=+5)\n",
    "    per_2.append(date.strftime(\"%-m/%-d/%y\"))\n",
    "    \n",
    "panel_2 = mergePanels(df_census_curated, per_2, weekly=True, datatype='death_rate')\n",
    "panel_2.to_csv('../data/panel_merged_period_2_weekly_norm_diff_deaths.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Tutorials)",
   "language": "python",
   "name": "pycharm-38c7cf03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
